{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us label the cells where the adversary can move below as cells 1 through 8. Cell number 6 is the starting cell for our adversary. The adversary can only randomly move to its adjacent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 5\n",
    "m=8\n",
    "Game_Grid = np.zeros((n, n))\n",
    "Game_Grid[1, 1] = 1  #State 1\n",
    "Game_Grid[1, 2] = 2  #State 2\n",
    "Game_Grid[1, 3] = 3  #State 3\n",
    "Game_Grid[2, 1] = 4  #State 4\n",
    "Game_Grid[2, 3] = 5  #State 5\n",
    "Game_Grid[3, 1] = 6  #State 6\n",
    "Game_Grid[3, 2] = 7  #State 7\n",
    "Game_Grid[3, 3] = 8  #State 8\n",
    "\n",
    "Game_Grid_Inv={}\n",
    "Game_Grid_Inv[1]=(1,1)\n",
    "Game_Grid_Inv[2]=(1,2)\n",
    "Game_Grid_Inv[3]=(1,3)\n",
    "Game_Grid_Inv[4]=(2,1)\n",
    "Game_Grid_Inv[5]=(2,3)\n",
    "Game_Grid_Inv[6]=(3,1)\n",
    "Game_Grid_Inv[7]=(3,2)\n",
    "Game_Grid_Inv[8]=(3,3)\n",
    "\n",
    "Game_Grid\n",
    "#Game_Grid_Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_Grid_Inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trans_Matrix will be our transition probability matrix for the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.sum(Game_Grid > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,3] = 1\n",
    "Trans_Matrix[1,0] = 1\n",
    "Trans_Matrix[2,1] = 1\n",
    "Trans_Matrix[3,5] = 1\n",
    "Trans_Matrix[4,2] = 1\n",
    "Trans_Matrix[5,6] = 1\n",
    "Trans_Matrix[6,7] = 1\n",
    "Trans_Matrix[7,4] = 1\n",
    "#Trans_Matrix = np.linalg.inv(Trans_Matrix)\n",
    "Trans_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=5\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), 5-int(i[1]))\n",
    "    \n",
    "nx.draw(G,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    #print(i)\n",
    "    G.nodes[i]['reward']=False\n",
    "    G.nodes[i]['possible_adversary']=False\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    G.nodes[i]['current_adversary']=False\n",
    "G.nodes[(2,2)]['reward']=True\n",
    "adversaryMoveset=[(1,1), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2), (3,3)]\n",
    "for i in adversaryMoveset:\n",
    "    G.nodes[i]['possible_adversary']=True\n",
    "\n",
    "starting_adversary_location=(1,1)\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "G.nodes[starting_adversary_location]['current_adversary']=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G,pos)\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if i==starting_agent_location], node_color='black')    \n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['reward']], node_color='g')    \n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['possible_adversary']], node_color='y')\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['current_adversary']], node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dimension=5\n",
    "starting_agent_location=(0,0)\n",
    "dest=(4,4)\n",
    "rew=(2,2)\n",
    "T=grid_dimension*grid_dimension\n",
    "done=False\n",
    "captured=False\n",
    "maxReward=100\n",
    "\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location=starting_adversary_location\n",
    "#current_location=starting_location\n",
    "current_time=0\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main online optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,5):\n",
    "    for k in range(0,5):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])\n",
    "for e in range(50):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,5):\n",
    "        for k in range(0,5):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    captured=False\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location=starting_adversary_location\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc=int(Game_Grid[current_adversary_location[1],current_adversary_location[0]])\n",
    "        #print(\"Current agent location: \"+str(current_agent_location))\n",
    "        #print(\"Current adversary location: \"+str(current_adversary_location)+\" or \"+str(current_adversary_loc))\n",
    "        #print(current_adversary_loc)\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .01\n",
    "\n",
    "        if current_agent_location==current_adversary_location:\n",
    "            #print(\"Captured\")\n",
    "            break\n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (294-reward[e])\n",
    "            #print(\"Successful\")\n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        if captured:\n",
    "            destination=dest\n",
    "        else:\n",
    "            destination=rew \n",
    "        #print(destination)\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True and not captured:\n",
    "                r[i]=maxReward\n",
    "            else:\n",
    "                r[i]=0\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            original_probabilities = np.linalg.matrix_power(Trans_Matrix, t+1-current_time)[current_adversary_loc-1,] # probabilities using matrix power function \n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['possible_adversary']==True:\n",
    "                    p[i][t]=original_probabilities[int(Game_Grid[i[1],i[0]])-1]\n",
    "                else:\n",
    "                    p[i][t]=0\n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # constraints (2b)\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "         # constraints (2b)\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # constraints (2c)\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # constraints (2d)\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # constraint (2e)\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        # constraint (2f)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if x[i,j][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if x[j,i][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True:\n",
    "                if str(i)==str(next_location):\n",
    "                    captured=True\n",
    "                    #print(\"Just captured\")\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        from random import choices\n",
    "        choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "        #print(choose)\n",
    "        next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "        ########################################\n",
    "\n",
    "        current_adversary_location=next_adversary_location\n",
    "        #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        if current_agent_location == current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location == (2,2):\n",
    "            if agent_heatmap[current_agent_location] == 0:\n",
    "                reward[e] += 200\n",
    "            else:\n",
    "                reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "            \n",
    "            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = 5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.array([\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "    ])\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "n = 5\n",
    "m=8\n",
    "Game_Grid = np.zeros((n, n))\n",
    "Game_Grid[1, 1] = 1  #State 1\n",
    "Game_Grid[1, 2] = 2  #State 2\n",
    "Game_Grid[1, 3] = 3  #State 3\n",
    "Game_Grid[2, 1] = 4  #State 4\n",
    "Game_Grid[2, 3] = 5  #State 5\n",
    "Game_Grid[3, 1] = 6  #State 6\n",
    "Game_Grid[3, 2] = 7  #State 7\n",
    "Game_Grid[3, 3] = 8  #State 8\n",
    "\n",
    "Game_Grid_Inv={}\n",
    "Game_Grid_Inv[1]=(1,1)\n",
    "Game_Grid_Inv[2]=(1,2)\n",
    "Game_Grid_Inv[3]=(1,3)\n",
    "Game_Grid_Inv[4]=(2,1)\n",
    "Game_Grid_Inv[5]=(2,3)\n",
    "Game_Grid_Inv[6]=(3,1)\n",
    "Game_Grid_Inv[7]=(3,2)\n",
    "Game_Grid_Inv[8]=(3,3)\n",
    "\n",
    "m = np.sum(Game_Grid > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,3] = 1\n",
    "Trans_Matrix[1,0] = 1\n",
    "Trans_Matrix[2,1] = 1\n",
    "Trans_Matrix[3,5] = 1\n",
    "Trans_Matrix[4,2] = 1\n",
    "Trans_Matrix[5,6] = 1\n",
    "Trans_Matrix[6,7] = 1\n",
    "Trans_Matrix[7,4] = 1\n",
    "\n",
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=5\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), 5-int(i[1]))\n",
    "    \n",
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    #print(i)\n",
    "    G.nodes[i]['reward']=False\n",
    "    G.nodes[i]['possible_adversary']=False\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    G.nodes[i]['current_adversary']=False\n",
    "G.nodes[(2,2)]['reward']=True\n",
    "adversaryMoveset=[(1,1), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2), (3,3)]\n",
    "for i in adversaryMoveset:\n",
    "    G.nodes[i]['possible_adversary']=True\n",
    "\n",
    "starting_adversary_location=(1,1)\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "G.nodes[starting_adversary_location]['current_adversary']=True\n",
    "\n",
    "grid_dimension=5\n",
    "starting_agent_location=(0,0)\n",
    "dest=(4,4)\n",
    "rew=(2,2)\n",
    "T=grid_dimension*grid_dimension\n",
    "done=False\n",
    "captured=False\n",
    "maxReward=100\n",
    "\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location=starting_adversary_location\n",
    "#current_location=starting_location\n",
    "current_time=0\n",
    "current_adversary_loc = (int(Game_Grid[current_adversary_location[1],current_adversary_location[0]]))\n",
    "\n",
    "from random import choices\n",
    "y = 0\n",
    "m = np.sum(Game_Grid > 0)\n",
    "obs_trans_matrix = np.zeros((m,m))\n",
    "for f in range(0,8):\n",
    "    choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "    next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "    next_adversary_loc=int(Game_Grid[next_adversary_location[1],next_adversary_location[0]])\n",
    "    obs_trans_matrix[current_adversary_loc-1,next_adversary_loc-1] += 1\n",
    "    current_adversary_loc = next_adversary_loc\n",
    "    current_adversary_location = next_adversary_location\n",
    "    \n",
    "for i in range(m):\n",
    "    obs_trans_matrix[i, ] = obs_trans_matrix[i, ]/np.sum(obs_trans_matrix[i,:])\n",
    "\n",
    "from collections import Counter\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,5):\n",
    "    for k in range(0,5):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])\n",
    "for e in range(50):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,5):\n",
    "        for k in range(0,5):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    captured=False\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location=starting_adversary_location\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc=int(Game_Grid[current_adversary_location[1],current_adversary_location[0]])\n",
    "        #print(\"Current agent location: \"+str(current_agent_location))\n",
    "        #print(\"Current adversary location: \"+str(current_adversary_location)+\" or \"+str(current_adversary_loc))\n",
    "        #print(current_adversary_loc)\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .01\n",
    "\n",
    "        if current_agent_location==current_adversary_location:\n",
    "            #print(\"Captured\")\n",
    "            break\n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (294-reward[e])\n",
    "            #print(\"Successful\")\n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        if captured:\n",
    "            destination=dest\n",
    "        else:\n",
    "            destination=rew \n",
    "        #print(destination)\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True and not captured:\n",
    "                r[i]=maxReward\n",
    "            else:\n",
    "                r[i]=0\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            original_probabilities = np.linalg.matrix_power(obs_trans_matrix, t+1-current_time)[current_adversary_loc-1,] # probabilities using matrix power function \n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['possible_adversary']==True:\n",
    "                    p[i][t]=original_probabilities[int(Game_Grid[i[1],i[0]])-1]\n",
    "                else:\n",
    "                    p[i][t]=0\n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # constraints (2b)\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "         # constraints (2b)\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # constraints (2c)\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # constraints (2d)\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # constraint (2e)\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        # constraint (2f)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if x[i,j][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if x[j,i][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True:\n",
    "                if str(i)==str(next_location):\n",
    "                    captured=True\n",
    "                    #print(\"Just captured\")\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "        #print(choose)\n",
    "        next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "        ########################################\n",
    "\n",
    "        current_adversary_location=next_adversary_location\n",
    "        #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        if current_agent_location == current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location == (2,2):\n",
    "            if agent_heatmap[current_agent_location] == 0:\n",
    "                reward[e] += 200\n",
    "            else:\n",
    "                reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "            \n",
    "            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = 5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.array([\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "    ])\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n",
    "print(\"time:\",time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = 5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.array([\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "    ])\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n",
    "print(\"time:\",time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
