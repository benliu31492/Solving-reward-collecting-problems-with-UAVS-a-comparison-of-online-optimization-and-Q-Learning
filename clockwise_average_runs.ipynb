{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us label the cells where the adversary can move below as cells 1 through 8. Cell number 6 is the starting cell for our adversary. The adversary can only randomly move to its adjacent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 1., 2., 3., 0.],\n",
       "       [0., 4., 0., 5., 0.],\n",
       "       [0., 6., 7., 8., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 5\n",
    "m=8\n",
    "Game_Grid = np.zeros((n, n))\n",
    "Game_Grid[1, 1] = 1  #State 1\n",
    "Game_Grid[1, 2] = 2  #State 2\n",
    "Game_Grid[1, 3] = 3  #State 3\n",
    "Game_Grid[2, 1] = 4  #State 4\n",
    "Game_Grid[2, 3] = 5  #State 5\n",
    "Game_Grid[3, 1] = 6  #State 6\n",
    "Game_Grid[3, 2] = 7  #State 7\n",
    "Game_Grid[3, 3] = 8  #State 8\n",
    "\n",
    "Game_Grid_Inv={}\n",
    "Game_Grid_Inv[1]=(1,1)\n",
    "Game_Grid_Inv[2]=(1,2)\n",
    "Game_Grid_Inv[3]=(1,3)\n",
    "Game_Grid_Inv[4]=(2,1)\n",
    "Game_Grid_Inv[5]=(2,3)\n",
    "Game_Grid_Inv[6]=(3,1)\n",
    "Game_Grid_Inv[7]=(3,2)\n",
    "Game_Grid_Inv[8]=(3,3)\n",
    "\n",
    "Game_Grid\n",
    "#Game_Grid_Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (1, 1),\n",
       " 2: (1, 2),\n",
       " 3: (1, 3),\n",
       " 4: (2, 1),\n",
       " 5: (2, 3),\n",
       " 6: (3, 1),\n",
       " 7: (3, 2),\n",
       " 8: (3, 3)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Game_Grid_Inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trans_Matrix will be our transition probability matrix for the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.sum(Game_Grid > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,1] = 1\n",
    "Trans_Matrix[1,2] = 1\n",
    "Trans_Matrix[2,4] = 1\n",
    "Trans_Matrix[3,0] = 1\n",
    "Trans_Matrix[4,7] = 1\n",
    "Trans_Matrix[5,3] = 1\n",
    "Trans_Matrix[6,5] = 1\n",
    "Trans_Matrix[7,6] = 1\n",
    "#Trans_Matrix = np.linalg.inv(Trans_Matrix)\n",
    "Trans_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gurobipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9dc2995479ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgurobipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gurobipy'"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=5\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), 5-int(i[1]))\n",
    "    \n",
    "nx.draw(G,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    #print(i)\n",
    "    G.nodes[i]['reward']=False\n",
    "    G.nodes[i]['possible_adversary']=False\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    G.nodes[i]['current_adversary']=False\n",
    "G.nodes[(2,2)]['reward']=True\n",
    "adversaryMoveset=[(1,1), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2), (3,3)]\n",
    "for i in adversaryMoveset:\n",
    "    G.nodes[i]['possible_adversary']=True\n",
    "\n",
    "starting_adversary_location=(1,1)\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "G.nodes[starting_adversary_location]['current_adversary']=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa7c145d3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(G,pos)\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if i==starting_agent_location], node_color='black')    \n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['reward']], node_color='g')    \n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['possible_adversary']], node_color='y')\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i]['current_adversary']], node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dimension=5\n",
    "starting_agent_location=(0,0)\n",
    "dest=(4,4)\n",
    "rew=(2,2)\n",
    "T=grid_dimension*grid_dimension\n",
    "done=False\n",
    "captured=False\n",
    "maxReward=100\n",
    "\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location=starting_adversary_location\n",
    "#current_location=starting_location\n",
    "current_time=0\n",
    "current_adversary_loc = (int(Game_Grid[current_adversary_location[1],current_adversary_location[0]]))\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main online optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of successes 1\n",
      "average reward: 294.0\n",
      "average regret: 0.0\n",
      "time:  0.9540450572967529\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEyklEQVR4nO3XMW4bWRpG0b8MZU4NMFU2G2BMcDValFZTqNDoDUzmBTikU9cEmrA1kmCyOVc4JyPeI/AlF3xc9n0f4P/fl3sPAN5HrBAhVogQK0SIFSLEChEPH7n87du3/fHx8UZTruvXr1/z9evXe894t9Le0taZ1t4fP37Mz58/l787+1Csj4+P8/379+usurFt2+Z8Pt97xrut65rZW9o609p7PB5fPfMMhgixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoWIh7cuLMvyNDNPMzOHw2G2bbv5qGu4XC6zruu9Z7xbaW9p60xv72vejHXf9+eZeZ6ZOR6P++l0uvmoa9i2bSpbZ1p7t22b8/l87xnvtq5rau9rPIMhQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIeLho1/48qXTd2lrze/fv+894UNqe//Om7Euy/I0M08zM4fDYdZ1vfWmq7hcLpmtMy97t22794x3KW2d6e19zZux7vv+PDPPMzPH43E/n8+33nQV67pOZevMy97T6XTvGe+ybVtm60xv72u8EyFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFgh4uGtC8uyPM3M08zM4XCYdV1vvekqLpdLZuvMy95t2+49411KW2d6e1/zZqz7vj/PzPPMzPF43M/n8603XcW6rlPZOtPaW9o687L3dDrde8Yf8wyGCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQsTDWxeWZXmamaeZmcPhMOu63nrTVVwul8zWmdbe0taZl73btt17xh97M9Z9359n5nlm5ng87ufz+dabrmJd16lsnWntLW2dedl7Op3uPeOPeQZDhFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQsTDWxeWZXmamaf/frwsy/Lv2066mm8z8/PeIz6gtLe0daa191+vHSz7vv+TQ/4xy7L8te/78d473qu0t7R1prX3f231DIYIsULEZ471+d4DPqi0t7R1prX31a2f9j8rfDaf+ZcVPhWxQoRYIUKsECFWiPgPGkj6aVn6OekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "from random import choices\n",
    "y = 0\n",
    "m = np.sum(Game_Grid > 0)\n",
    "obs_trans_matrix = np.zeros((m,m))\n",
    "for f in range(0,24):\n",
    "    choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "    next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "    next_adversary_loc=int(Game_Grid[next_adversary_location[1],next_adversary_location[0]])\n",
    "    obs_trans_matrix[current_adversary_loc-1,next_adversary_loc-1] += 1\n",
    "    current_adversary_loc = next_adversary_loc\n",
    "    current_adversary_location = next_adversary_location\n",
    "    \n",
    "for i in range(m):\n",
    "    obs_trans_matrix[i, ] = obs_trans_matrix[i, ]/np.sum(obs_trans_matrix[i,:])\n",
    "obs_trans_matrix\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,5):\n",
    "    for k in range(0,5):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])\n",
    "for e in range(1):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,5):\n",
    "        for k in range(0,5):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    captured=False\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location=starting_adversary_location\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc=int(Game_Grid[current_adversary_location[1],current_adversary_location[0]])\n",
    "        #print(\"Current agent location: \"+str(current_agent_location))\n",
    "        #print(\"Current adversary location: \"+str(current_adversary_location)+\" or \"+str(current_adversary_loc))\n",
    "        #print(current_adversary_loc)\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .01\n",
    "\n",
    "        if current_agent_location==current_adversary_location:\n",
    "            #print(\"Captured\")\n",
    "            break\n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (294-reward[e])\n",
    "            #print(\"Successful\")\n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        if captured:\n",
    "            destination=dest\n",
    "        else:\n",
    "            destination=rew \n",
    "        #print(destination)\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True and not captured:\n",
    "                r[i]=maxReward\n",
    "            else:\n",
    "                r[i]=0\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            original_probabilities = np.linalg.matrix_power(obs_trans_matrix, t+1-current_time)[current_adversary_loc-1,] # probabilities using matrix power function \n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['possible_adversary']==True:\n",
    "                    p[i][t]=original_probabilities[int(Game_Grid[i[1],i[0]])-1]\n",
    "                    #print((Game_Grid[i[1],i[0]])-1)\n",
    "                else:\n",
    "                    p[i][t]=0\n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # constraints (2b)\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "         # constraints (2b)\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # constraints (2c)\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # constraints (2d)\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # constraint (2e)\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        # constraint (2f)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if x[i,j][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if x[j,i][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True:\n",
    "                if str(i)==str(next_location):\n",
    "                    captured=True\n",
    "                    #print(\"Just captured\")\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        from random import choices\n",
    "        choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "        #print(choose)\n",
    "        next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "        ########################################\n",
    "\n",
    "        current_adversary_location=next_adversary_location\n",
    "        #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        if current_agent_location == current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location == (2,2):\n",
    "            if agent_heatmap[current_agent_location] == 0:\n",
    "                reward[e] += 200\n",
    "            else:\n",
    "                reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "            \n",
    "            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = 5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.array([\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "    ])\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n",
    "print(\"time: \", time.time()-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using license file /Users/madeleine/gurobi.lic\n",
      "Academic license - for non-commercial use only\n",
      "number of successes 50\n",
      "average reward: 294.0\n",
      "average regret: 0.0\n",
      "time:  48.88817811012268\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "n = 5\n",
    "m=8\n",
    "Game_Grid = np.zeros((n, n))\n",
    "Game_Grid[1, 1] = 1  #State 1\n",
    "Game_Grid[1, 2] = 2  #State 2\n",
    "Game_Grid[1, 3] = 3  #State 3\n",
    "Game_Grid[2, 1] = 4  #State 4\n",
    "Game_Grid[2, 3] = 5  #State 5\n",
    "Game_Grid[3, 1] = 6  #State 6\n",
    "Game_Grid[3, 2] = 7  #State 7\n",
    "Game_Grid[3, 3] = 8  #State 8\n",
    "\n",
    "Game_Grid_Inv={}\n",
    "Game_Grid_Inv[1]=(1,1)\n",
    "Game_Grid_Inv[2]=(1,2)\n",
    "Game_Grid_Inv[3]=(1,3)\n",
    "Game_Grid_Inv[4]=(2,1)\n",
    "Game_Grid_Inv[5]=(2,3)\n",
    "Game_Grid_Inv[6]=(3,1)\n",
    "Game_Grid_Inv[7]=(3,2)\n",
    "Game_Grid_Inv[8]=(3,3)\n",
    "\n",
    "m = np.sum(Game_Grid > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,1] = 1\n",
    "Trans_Matrix[1,2] = 1\n",
    "Trans_Matrix[2,4] = 1\n",
    "Trans_Matrix[3,0] = 1\n",
    "Trans_Matrix[4,7] = 1\n",
    "Trans_Matrix[5,3] = 1\n",
    "Trans_Matrix[6,5] = 1\n",
    "Trans_Matrix[7,6] = 1\n",
    "\n",
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=5\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), 5-int(i[1]))\n",
    "    \n",
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    #print(i)\n",
    "    G.nodes[i]['reward']=False\n",
    "    G.nodes[i]['possible_adversary']=False\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    G.nodes[i]['current_adversary']=False\n",
    "G.nodes[(2,2)]['reward']=True\n",
    "adversaryMoveset=[(1,1), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2), (3,3)]\n",
    "for i in adversaryMoveset:\n",
    "    G.nodes[i]['possible_adversary']=True\n",
    "\n",
    "starting_adversary_location=(1,1)\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "G.nodes[starting_adversary_location]['current_adversary']=True\n",
    "\n",
    "grid_dimension=5\n",
    "starting_agent_location=(0,0)\n",
    "dest=(4,4)\n",
    "rew=(2,2)\n",
    "T=grid_dimension*grid_dimension\n",
    "done=False\n",
    "captured=False\n",
    "maxReward=100\n",
    "\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location=starting_adversary_location\n",
    "#current_location=starting_location\n",
    "current_time=0\n",
    "current_adversary_loc = (int(Game_Grid[current_adversary_location[1],current_adversary_location[0]]))\n",
    "\n",
    "from random import choices\n",
    "y = 0\n",
    "m = np.sum(Game_Grid > 0)\n",
    "obs_trans_matrix = np.zeros((m,m))\n",
    "for f in range(0,8):\n",
    "    choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "    next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "    next_adversary_loc=int(Game_Grid[next_adversary_location[1],next_adversary_location[0]])\n",
    "    obs_trans_matrix[current_adversary_loc-1,next_adversary_loc-1] += 1\n",
    "    current_adversary_loc = next_adversary_loc\n",
    "    current_adversary_location = next_adversary_location\n",
    "    \n",
    "for i in range(m):\n",
    "    obs_trans_matrix[i, ] = obs_trans_matrix[i, ]/np.sum(obs_trans_matrix[i,:])\n",
    "\n",
    "from collections import Counter\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,5):\n",
    "    for k in range(0,5):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "#print(current_agent_location[0], current_agent_location[1])\n",
    "for e in range(50):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,5):\n",
    "        for k in range(0,5):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    captured=False\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location=starting_adversary_location\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc=int(Game_Grid[current_adversary_location[1],current_adversary_location[0]])\n",
    "        #print(\"Current agent location: \"+str(current_agent_location))\n",
    "        #print(\"Current adversary location: \"+str(current_adversary_location)+\" or \"+str(current_adversary_loc))\n",
    "        #print(current_adversary_loc)\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .01\n",
    "\n",
    "        if current_agent_location==current_adversary_location:\n",
    "            #print(\"Captured\")\n",
    "            break\n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (294-reward[e])\n",
    "            #print(\"Successful\")\n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        if captured:\n",
    "            destination=dest\n",
    "        else:\n",
    "            destination=rew \n",
    "        #print(destination)\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True and not captured:\n",
    "                r[i]=maxReward\n",
    "            else:\n",
    "                r[i]=0\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            original_probabilities = np.linalg.matrix_power(obs_trans_matrix, t+1-current_time)[current_adversary_loc-1,] # probabilities using matrix power function \n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['possible_adversary']==True:\n",
    "                    p[i][t]=original_probabilities[int(Game_Grid[i[1],i[0]])-1]\n",
    "                    #print((Game_Grid[i[1],i[0]])-1)\n",
    "                else:\n",
    "                    p[i][t]=0\n",
    "        \n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # constraints (2b)\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "         # constraints (2b)\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # constraints (2c)\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # constraints (2d)\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # constraint (2e)\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        # constraint (2f)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if x[i,j][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if x[j,i][t].X==1:\n",
    "                    #print(t)\n",
    "                    #print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            if d['reward']==True:\n",
    "                if str(i)==str(next_location):\n",
    "                    captured=True\n",
    "                    #print(\"Just captured\")\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "        #print(choose)\n",
    "        next_adversary_location=(Game_Grid_Inv[choose[0]+1][1],Game_Grid_Inv[choose[0]+1][0])\n",
    "        ########################################\n",
    "\n",
    "        current_adversary_location=next_adversary_location\n",
    "        #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        if current_agent_location == current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location == (2,2):\n",
    "            if agent_heatmap[current_agent_location] == 0:\n",
    "                reward[e] += 200\n",
    "            else:\n",
    "                reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "            \n",
    "            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = 5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.array([\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "        [ 1.,  1.,  1.,  1.,  1.],\n",
    "    ])\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n",
    "print(\"time: \", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
