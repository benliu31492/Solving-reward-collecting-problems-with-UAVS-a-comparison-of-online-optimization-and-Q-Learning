{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us label the cells where the adversary can move below as cells 1 through 8. Cell number 6 is the starting cell for our adversary. The adversary can only randomly move to its adjacent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 2., 3., 0., 0., 0., 0., 0., 0.],\n",
       "        [4., 0., 5., 0., 0., 0., 0., 0., 0.],\n",
       "        [6., 7., 8., 0., 0., 0., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 1., 2., 3.],\n",
       "        [0., 0., 0., 0., 0., 0., 4., 0., 5.],\n",
       "        [0., 0., 0., 0., 0., 0., 6., 7., 8.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Game_Grid = []\n",
    "Game_Grid_Inv = []\n",
    "rewards = [(7,1),(1,7)]\n",
    "maze_size = 9\n",
    "for reward in rewards: \n",
    "    \n",
    "    game_grid = np.zeros((maze_size,maze_size))\n",
    "    \n",
    "    game_grid[reward[0]-1, reward[1]-1] = 1  #State 1\n",
    "    game_grid[reward[0]-1, reward[1]] = 2  #State 2\n",
    "    game_grid[reward[0]-1, reward[1]+1] = 3  #State 3\n",
    "    game_grid[reward[0], reward[1]-1] = 4  #State 4\n",
    "    game_grid[reward[0], reward[1]+1] = 5  #State 5\n",
    "    game_grid[reward[0]+1, reward[1]-1] = 6  #State 6\n",
    "    game_grid[reward[0]+1, reward[1]] = 7  #State 7\n",
    "    game_grid[reward[0]+1, reward[1]+1] = 8  #State 8\n",
    "    Game_Grid.append(game_grid) \n",
    "    \n",
    "    game_grid_inv={}\n",
    "\n",
    "    game_grid_inv[1]=(reward[0]-1,reward[1]-1)\n",
    "    game_grid_inv[2]=(reward[0]-1,reward[1])\n",
    "    game_grid_inv[3]=(reward[0]-1,reward[1]+1)\n",
    "    game_grid_inv[4]=(reward[0],reward[1]-1)\n",
    "    game_grid_inv[5]=(reward[0],reward[1]+1)\n",
    "    game_grid_inv[6]=(reward[0]+1,reward[1]-1)\n",
    "    game_grid_inv[7]=(reward[0]+1,reward[1])\n",
    "    game_grid_inv[8]=(reward[0]+1,reward[1]+1)\n",
    "    Game_Grid_Inv.append(game_grid_inv)\n",
    "    \n",
    "Game_Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: (6, 0),\n",
       "  2: (6, 1),\n",
       "  3: (6, 2),\n",
       "  4: (7, 0),\n",
       "  5: (7, 2),\n",
       "  6: (8, 0),\n",
       "  7: (8, 1),\n",
       "  8: (8, 2)},\n",
       " {1: (0, 6),\n",
       "  2: (0, 7),\n",
       "  3: (0, 8),\n",
       "  4: (1, 6),\n",
       "  5: (1, 8),\n",
       "  6: (2, 6),\n",
       "  7: (2, 7),\n",
       "  8: (2, 8)}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Game_Grid_Inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trans_Matrix will be our transition probability matrix for the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0. , 0.5, 0. , 0. , 0. , 0. ],\n",
       "       [0.5, 0. , 0.5, 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.5, 0. , 0. , 0.5, 0. , 0. , 0. ],\n",
       "       [0.5, 0. , 0. , 0. , 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0.5, 0. , 0. , 0.5, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0.5, 0. , 0.5, 0. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.sum(Game_Grid[0] > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,1] = .5\n",
    "Trans_Matrix[0,3] = .5\n",
    "Trans_Matrix[1,0] = .5\n",
    "Trans_Matrix[1,2] = .5\n",
    "Trans_Matrix[2,1] = .5\n",
    "Trans_Matrix[2,4] = .5\n",
    "Trans_Matrix[3,0] = .5\n",
    "Trans_Matrix[3,5] = .5\n",
    "Trans_Matrix[4,2] = .5\n",
    "Trans_Matrix[4,7] = .5\n",
    "Trans_Matrix[5,3] = .5\n",
    "Trans_Matrix[5,6] = .5\n",
    "Trans_Matrix[6,5] = .5\n",
    "Trans_Matrix[6,7] = .5\n",
    "Trans_Matrix[7,4] = .5\n",
    "Trans_Matrix[7,6] = .5\n",
    "#Trans_Matrix = np.linalg.inv(Trans_Matrix)\n",
    "Trans_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madeleine/opt/anaconda3/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=maze_size\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), maze_size-int(i[1]))\n",
    "    \n",
    "nx.draw(G,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    for j in range(0,len(rewards)):\n",
    "        current_adversary = 'current_adversary' + str(j)\n",
    "        G.nodes[i][current_adversary] = False\n",
    "        reward = 'reward'+ str(j)\n",
    "        G.nodes[i][reward] = False\n",
    "        possible_adversary = 'possible_adversary' + str(j)\n",
    "        G.nodes[i][possible_adversary] = False\n",
    "for j in range(0,len(rewards)):\n",
    "    reward = 'reward'+ str(j)\n",
    "    G.nodes[rewards[j]][reward]=True\n",
    "    possible_adversary = 'possible_adversary' + str(j)\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]-1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0],rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]-1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0],rewards[j][1]-1][possible_adversary] = True\n",
    "    \n",
    "\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "for j in range(0,len(rewards)):\n",
    "    current_adversary = 'current_adversary' + str(j)\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]-1][current_adversary]=True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(G,pos)\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if i==starting_agent_location], node_color='black') \n",
    "for j in range(0,len(rewards)):\n",
    "    reward = 'reward' + str(j)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i][reward]], node_color='g')    \n",
    "    possible_adversary = 'possible_adversary' + str(j)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i][possible_adversary]], node_color='y')\n",
    "    current_adversary = 'current_adversary' + str(j)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[i for i in G.nodes() if G.nodes[i][current_adversary]], node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_agent_location=(0,0)\n",
    "dest=(maze_size-1,maze_size-1)\n",
    "T=maze_size*maze_size\n",
    "done=False\n",
    "captured=[False]*len(rewards)\n",
    "maxReward=100\n",
    "\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location = []\n",
    "for i in range(0,len(rewards)):\n",
    "    current_adversary_location.append((rewards[i][1]-1,rewards[i][0]-1))\n",
    "current_time=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main online optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current agent location: (0, 0)\n",
      "Current adversary location: [(0, 6), (6, 0)]\n",
      "Current agent location: (0, 1)\n",
      "Current adversary location: [(1, 6), (6, 1)]\n",
      "Current agent location: (0, 2)\n",
      "Current adversary location: [(0, 6), (6, 0)]\n",
      "Current agent location: (1, 2)\n",
      "Current adversary location: [(1, 6), (7, 0)]\n",
      "Current agent location: (2, 2)\n",
      "Current adversary location: [(2, 6), (8, 0)]\n",
      "Current agent location: (2, 1)\n",
      "Current adversary location: [(1, 6), (7, 0)]\n",
      "Current agent location: (2, 2)\n",
      "Current adversary location: [(0, 6), (8, 0)]\n",
      "Current agent location: (2, 1)\n",
      "Current adversary location: [(1, 6), (7, 0)]\n",
      "Current agent location: (3, 1)\n",
      "Current adversary location: [(2, 6), (8, 0)]\n",
      "Current agent location: (4, 1)\n",
      "Current adversary location: [(2, 7), (7, 0)]\n",
      "Current agent location: (4, 2)\n",
      "Current adversary location: [(2, 8), (6, 0)]\n",
      "Current agent location: (5, 2)\n",
      "Current adversary location: [(2, 7), (6, 1)]\n",
      "Current agent location: (5, 3)\n",
      "Current adversary location: [(2, 6), (6, 0)]\n",
      "Current agent location: (6, 3)\n",
      "Current adversary location: [(1, 6), (6, 1)]\n",
      "Current agent location: (7, 3)\n",
      "Current adversary location: [(2, 6), (6, 0)]\n",
      "Current agent location: (7, 2)\n",
      "Current adversary location: [(2, 7), (7, 0)]\n",
      "Just captured reward 0\n",
      "Current agent location: (7, 1)\n",
      "Current adversary location: [(2, 8), (6, 0)]\n",
      "Current agent location: (7, 2)\n",
      "Current adversary location: [(1, 8), (6, 1)]\n",
      "Current agent location: (7, 3)\n",
      "Current adversary location: [(2, 8), (6, 2)]\n",
      "Current agent location: (6, 3)\n",
      "Current adversary location: [(2, 7), (7, 2)]\n",
      "Current agent location: (6, 4)\n",
      "Current adversary location: [(2, 6), (8, 2)]\n",
      "Current agent location: (6, 5)\n",
      "Current adversary location: [(1, 6), (7, 2)]\n",
      "Current agent location: (5, 5)\n",
      "Current adversary location: [(0, 6), (8, 2)]\n",
      "Current agent location: (5, 6)\n",
      "Current adversary location: [(1, 6), (7, 2)]\n",
      "Current agent location: (4, 6)\n",
      "Current adversary location: [(0, 6), (6, 2)]\n",
      "Current agent location: (4, 7)\n",
      "Current adversary location: [(1, 6), (7, 2)]\n",
      "Current agent location: (3, 7)\n",
      "Current adversary location: [(2, 6), (8, 2)]\n",
      "Current agent location: (4, 7)\n",
      "Current adversary location: [(2, 7), (8, 1)]\n",
      "Current agent location: (4, 6)\n",
      "Current adversary location: [(2, 8), (8, 0)]\n",
      "Current agent location: (4, 5)\n",
      "Current adversary location: [(2, 7), (8, 1)]\n",
      "Current agent location: (3, 5)\n",
      "Current adversary location: [(2, 8), (8, 2)]\n",
      "Current agent location: (2, 5)\n",
      "Current adversary location: [(1, 8), (8, 1)]\n",
      "Current agent location: (1, 5)\n",
      "Current adversary location: [(2, 8), (8, 0)]\n",
      "Current agent location: (1, 6)\n",
      "Current adversary location: [(2, 7), (7, 0)]\n",
      "Just captured reward 1\n",
      "Current agent location: (1, 7)\n",
      "Current adversary location: [(2, 6), (8, 0)]\n",
      "Current agent location: (1, 8)\n",
      "Current adversary location: [(1, 6), (8, 1)]\n",
      "Current agent location: (2, 8)\n",
      "Current adversary location: [(0, 6), (8, 0)]\n",
      "Current agent location: (3, 8)\n",
      "Current adversary location: [(1, 6), (8, 1)]\n",
      "Current agent location: (4, 8)\n",
      "Current adversary location: [(0, 6), (8, 2)]\n",
      "Current agent location: (5, 8)\n",
      "Current adversary location: [(0, 7), (8, 1)]\n",
      "Current agent location: (6, 8)\n",
      "Current adversary location: [(0, 6), (8, 0)]\n",
      "Current agent location: (7, 8)\n",
      "Current adversary location: [(0, 7), (8, 1)]\n",
      "Current agent location: (8, 8)\n",
      "Current adversary location: [(0, 6), (8, 2)]\n",
      "time:  360.8353281021118\n",
      "number of successes 1\n",
      "average reward: 461.0\n",
      "average regret: 14.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFI0lEQVR4nO3dMW5TaRiG0d8jOhjRREqTgi4LuLMmVpCWHbCCWQG7gAUkC0hHQRMppanvNDQjhYQA/7Ufc47kCvT5xtYjoHnZres6gOP316EfAPgxYoUIsUKEWCFCrBAhVoh48ZzffHZ2tr5582bSo4zx9evX8fLlS/cPdH+L93D/cZ8/fx739/e7B39xXdcffi3Lss708eNH9w94f4v3cP9x3xp7sD9/DYYIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIeFasNzc3Y7fbTXvNNvv56/e3+A62MPs7ONjPtT7xXz7udru3Y4y3Y4zx+vXr5d27d9Me5vLycrx69Wra/bu7u/Hly5dp9y8uLtL3x5j/Hez3++n3b29vp92/uLgY5+fn0+5fXV2N6+vrX98NHmOsM1+zN1nfv38/9fnr97f4Dra4P/s7mMluMJwAsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIOKpY67vEYzxvLfK5r2VZpj//KXwHp+qodoPru7uzN2Vnb+KOMf8z2mKX2G7wBrvB9d3d2Zuyszdxt/iM7AY/zm4wnACxQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0Q8azf4/Px8+fDhw7SH2e/30zdl3X/c3d1dfru5vA3923aDv22aTrPFpqz7jzuF7ebZ92eyGwwnQKwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWLlf5ZledY87XNfW5j5/MuybPIzPMTIt/ubvscWI+IzR7hnfz5Gvt0/mveoj3DP/nyMfMMJECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiKOK9ebmZux2u2kv9w//Hvy8o9oN3mJT1v3DvscW9+0Gb7AbvMWmrPun/TPYDQYOTqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIl4c+gG2tCzLWJ+YXv0Vnz59mn6/bovv4FT9UbvBl5eXUzdfZ2/K7vf7cXt7O+3+GPN3fU/hO7AbvM7fDZ69+brF/Zmfz9hg1/cUvoOZ7AbDCRArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoTd4N9o9q7v7E3fMez6Hvq+3eANN2tnPv/sz2erz8j977MbDCdArBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRf9Ru8Ozd3fr9Ld7D/cddXV2NdV3tBrt/+Pdw/+mX3WCIEytEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFCxLN2g8cYl2OM24nPczbGuHf/YPe3eA/3H3e5ruvfD/3Ck7FuabfbXa/r+o/7h7m/xXu4//P3/TUYIsQKEccW67/uH/T+Fu/h/k/eP6p/swLfd2x/sgLfIVaIECtEiBUixAoR/wHRo9ZPk7Dr0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "from collections import Counter\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,maze_size):\n",
    "    for k in range(0,maze_size):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "for e in range(2):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,maze_size):\n",
    "        for k in range(0,maze_size):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    captured=[False]*len(rewards)\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location = []  \n",
    "    for i in range(0,len(rewards)):\n",
    "        current_adversary_location.append((rewards[i][1]-1,rewards[i][0]-1))\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc = []\n",
    "        for z in range(0,len(rewards)):\n",
    "            current_adversary_loc.append(int(Game_Grid[z][current_adversary_location[z][1],current_adversary_location[z][0]]))\n",
    "        print(\"Current agent location: \"+str(current_agent_location))\n",
    "        print(\"Current adversary location: \"+str(current_adversary_location))\n",
    "        #print(\"Current adversary loc:\" + str(current_adversary_loc))\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .2\n",
    "\n",
    "\n",
    "        for z in range(0,len(rewards)):\n",
    "            if current_agent_location==current_adversary_location[z]:\n",
    "                #print(\"Captured Reward \"+str(z))\n",
    "                break\n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (475-reward[e])\n",
    "            #print(\"Successful\")\n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        #print(\"captured:\"+str(captured))\n",
    "        if all(captured):\n",
    "            destination=dest\n",
    "        else:\n",
    "            #rew = random.choice((np.where(np.array((captured))==False)[0]))\n",
    "            #print(captured)\n",
    "            rew = (np.where(np.array((captured))==False)[0])[0]\n",
    "            destination = (rewards[rew][0],rewards[rew][1])\n",
    "        #print(\"destination:\"+ str(destination))\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            r[i] = 0\n",
    "        for z in range(0,len(rewards)):\n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['reward'+str(z)]==True and not captured[z]:\n",
    "                    r[i]=maxReward\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                p[i][t]=0\n",
    "        for t in range(current_time, T):\n",
    "            for z in range(0,len(rewards)):\n",
    "                original_probabilities = np.linalg.matrix_power(Trans_Matrix, t+1-current_time)[current_adversary_loc[z]-1,] # probabilities using matrix power function \n",
    "                #for (i,d) in G.nodes(data=True):\n",
    "                #    if d['possible_adversary'+str(z)]==True:\n",
    "                #        p[i][t]=original_probabilities[int(Game_Grid[z][i[1]][i[0]])-1]\n",
    "                for i, e in enumerate(list(original_probabilities)):\n",
    "                    if e != 0:\n",
    "                        p[(Game_Grid_Inv[z][i+1][1],Game_Grid_Inv[z][i+1][0])][t] = original_probabilities[i]\n",
    "                        print(i,t,p[i][t])\n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # agent only be at one node at a time\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "        # agent can only be in each node once when planning\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # The agent can and will only exit from the node it is currently in\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # When an agent enters a node it must exit it as well\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # agent has to move every step\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        #have to reach destination(might not be needed)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if x[i,j][t].X==1:\n",
    "                    print(t)\n",
    "                    print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if x[j,i][t].X==1:\n",
    "                    print(t)\n",
    "                    print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            for z in range(0,len(rewards)):\n",
    "                if d['reward'+str(z)]==True:\n",
    "                    if str(i)==str(next_location):\n",
    "                        captured[z]=True\n",
    "                        print(\"Just captured reward \" + str(z))\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        from random import choices\n",
    "        next_adversary_location = []\n",
    "        for z in range(0,len(rewards)):\n",
    "            choose=choices(range(0,8), Trans_Matrix[current_adversary_loc[z]-1,:])\n",
    "            #print(choose)\n",
    "            next_adversary_location.append((Game_Grid_Inv[z][choose[0]+1][1],Game_Grid_Inv[z][choose[0]+1][0]))\n",
    "        ########################################\n",
    "\n",
    "        for z in range(0,len(rewards)):\n",
    "            current_adversary_location[z]=next_adversary_location[z]\n",
    "            #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        \n",
    "        if current_agent_location in current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location in rewards:\n",
    "            #if agent_heatmap[current_agent_location] == 0:\n",
    "            reward[e] += 200\n",
    "            #else:\n",
    "                #reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "\n",
    "            \n",
    "print(\"time: \",time.time()-start_time)            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = maze_size\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.full(\n",
    "                        shape=(maze_size,maze_size),\n",
    "                        fill_value=1,\n",
    "                        dtype=np.int)\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",total_regret/n_success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  16486.240142822266\n",
      "number of successes 25\n",
      "average reward: 454.76\n",
      "average regret: 20.24000000000001\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAE+ElEQVR4nO3dMU4baRzG4W9W6WBFg2SBXND5AJMz5QRuuYFPsCfgFvgATI87ikgIiRLqbxtW2gJMSPhm/A7PI7kieseW/FNI809Xay3A4ftr6jcA/BqxQgixQgixQgixQgixQohvH/nDp6en9eLiotFbKeX5+bkcHR3Z37N/e3vbbL+UUs7Ozsr9/b39Pfvn5+fN9u/u7srj42P36g9rrb/86vu+tnR9fW3/nf1SStPXZrOx/85+Sy+NvdqfX4MhhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghxIdOkTK92vh//dtut02fMYf9rnv9Umhr3XsfrOu6H6WUH6WUslgs+qurq2Zv5unpqRwfH9ufaH+MZ8xhf7fbNdtfr9el1upusP3pnzGH/dL4dnN1NxiyiRVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCiBVCHFSswzCUruuavSDZQd0Nfnh4KD9//my2v1qt4m/Wuhs8/b67wbXWzWbT9B7rHG7Wtpb+GdwNBiYnVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgjhbvAnSr+JO8Yz5rDvbnB1N3jq/TGeMYf9lt/R4m4w5BMrhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhBArhPhSsQ7DULqua/YahmHqj8iMfakj38vlsvn+YrFotu/I92HsO/Jd2x/5HmO/JUe+D2O/5XeoOPIN+cQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIb5N/QbG1Pd9qe+cXv0T2+222TZ8qbvBq9Uq/matu8HT77sbXNvfDZ7DzdrW0j+Du8HA5MQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIcQKIT4U6zAMpeu6Zq//7vq2esFnaPkd7fv+zed+6G7wyclJf3l5+akf/P/c9Z12f4xn2N9vvV6Xm5ubP78bXBrfS53DTdnk/TGeYX+/l9vc7gZDMrFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCHeD7Y/6jDH2d7tds/3lclkWi0WzfXeDX9if/hlj7Lf8jm42m6bv391gmAGxQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgh3g+2P+gz7+7kb/ML+9M+wv5+7wTADYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQH7obXEpZlVJ2Dd/PaSnl0f5k+2M8w/5+q1rr36/94N1Yx9R13U2t9bv9afbHeIb939/3azCEECuEOLRY/7E/6f4Yz7D/m/sH9W9W4G2H9jcr8AaxQgixQgixQgixQoh/Ad9xOJ+UyuPdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "Game_Grid = []\n",
    "Game_Grid_Inv = []\n",
    "rewards = [(7,1),(1,7)]\n",
    "maze_size = 9\n",
    "for reward in rewards: \n",
    "    \n",
    "    game_grid = np.zeros((maze_size,maze_size))\n",
    "    \n",
    "    game_grid[reward[0]-1, reward[1]-1] = 1  #State 1\n",
    "    game_grid[reward[0]-1, reward[1]] = 2  #State 2\n",
    "    game_grid[reward[0]-1, reward[1]+1] = 3  #State 3\n",
    "    game_grid[reward[0], reward[1]-1] = 4  #State 4\n",
    "    game_grid[reward[0], reward[1]+1] = 5  #State 5\n",
    "    game_grid[reward[0]+1, reward[1]-1] = 6  #State 6\n",
    "    game_grid[reward[0]+1, reward[1]] = 7  #State 7\n",
    "    game_grid[reward[0]+1, reward[1]+1] = 8  #State 8\n",
    "    Game_Grid.append(game_grid) \n",
    "    \n",
    "    game_grid_inv={}\n",
    "\n",
    "    game_grid_inv[1]=(reward[0]-1,reward[1]-1)\n",
    "    game_grid_inv[2]=(reward[0]-1,reward[1])\n",
    "    game_grid_inv[3]=(reward[0]-1,reward[1]+1)\n",
    "    game_grid_inv[4]=(reward[0],reward[1]-1)\n",
    "    game_grid_inv[5]=(reward[0],reward[1]+1)\n",
    "    game_grid_inv[6]=(reward[0]+1,reward[1]-1)\n",
    "    game_grid_inv[7]=(reward[0]+1,reward[1])\n",
    "    game_grid_inv[8]=(reward[0]+1,reward[1]+1)\n",
    "    Game_Grid_Inv.append(game_grid_inv)\n",
    "    \n",
    "m = np.sum(Game_Grid[0] > 0)\n",
    "Trans_Matrix = np.zeros((m, m))\n",
    "Trans_Matrix[0,1] = .5\n",
    "Trans_Matrix[0,3] = .5\n",
    "Trans_Matrix[1,0] = .5\n",
    "Trans_Matrix[1,2] = .5\n",
    "Trans_Matrix[2,1] = .5\n",
    "Trans_Matrix[2,4] = .5\n",
    "Trans_Matrix[3,0] = .5\n",
    "Trans_Matrix[3,5] = .5\n",
    "Trans_Matrix[4,2] = .5\n",
    "Trans_Matrix[4,7] = .5\n",
    "Trans_Matrix[5,3] = .5\n",
    "Trans_Matrix[5,6] = .5\n",
    "Trans_Matrix[6,5] = .5\n",
    "Trans_Matrix[6,7] = .5\n",
    "Trans_Matrix[7,4] = .5\n",
    "Trans_Matrix[7,6] = .5\n",
    "\n",
    "from gurobipy import *\n",
    "from itertools import combinations \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def Manhattan_Distance(x,y):\n",
    "    return np.abs(x[0]-y[0])+np.abs(x[1]-y[1])\n",
    "\n",
    "## Construct network\n",
    "grid_dimension=maze_size\n",
    "G=nx.Graph()\n",
    "for i in range(grid_dimension):\n",
    "    for j in range(grid_dimension):\n",
    "        G.add_node((i,j))\n",
    "        \n",
    "for (i,j) in combinations(G.nodes(),2):\n",
    "     if Manhattan_Distance(i,j)<=1:\n",
    "            G.add_edge(i,j)\n",
    "pos={}\n",
    "for i in G.nodes():\n",
    "    pos[i]=(int(i[0]), maze_size-int(i[1]))\n",
    "    \n",
    "starting_agent_location=(0,0)\n",
    "for i in G.nodes():\n",
    "    G.nodes[i]['current_agent']=False\n",
    "    for j in range(0,len(rewards)):\n",
    "        current_adversary = 'current_adversary' + str(j)\n",
    "        G.nodes[i][current_adversary] = False\n",
    "        rewardz = 'reward'+ str(j)\n",
    "        G.nodes[i][rewardz] = False\n",
    "        possible_adversary = 'possible_adversary' + str(j)\n",
    "        G.nodes[i][possible_adversary] = False\n",
    "for j in range(0,len(rewards)):\n",
    "    rewardz = 'reward'+ str(j)\n",
    "    G.nodes[rewards[j]][rewardz]=True\n",
    "    possible_adversary = 'possible_adversary' + str(j)\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]-1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0],rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]+1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0]+1,rewards[j][1]-1][possible_adversary] = True\n",
    "    G.nodes[rewards[j][0],rewards[j][1]-1][possible_adversary] = True\n",
    "    \n",
    "\n",
    "G.nodes[starting_agent_location]['current_agent']=True\n",
    "for j in range(0,len(rewards)):\n",
    "    current_adversary = 'current_adversary' + str(j)\n",
    "    G.nodes[rewards[j][0]-1,rewards[j][1]-1][current_adversary]=True\n",
    "    \n",
    "starting_agent_location=(0,0)\n",
    "dest=(maze_size-1,maze_size-1)\n",
    "T=(maze_size*maze_size)+10\n",
    "done=False\n",
    "captured=[False]*len(rewards)\n",
    "maxReward=100\n",
    "\n",
    "y = 0\n",
    "current_agent_location=starting_agent_location\n",
    "current_adversary_location = ((rewards[y][1]-1,rewards[y][0]-1))\n",
    "current_time=0\n",
    "current_adversary_loc = (int(Game_Grid[y][current_adversary_location[1],current_adversary_location[0]]))\n",
    "\n",
    "#from random import choices\n",
    "#y = 0\n",
    "#m = np.sum(Game_Grid[y] > 0)\n",
    "#obs_trans_matrix = np.zeros((m,m))\n",
    "#for f in range(0,75):\n",
    "#    choose=choices(range(0,8), Trans_Matrix[current_adversary_loc-1,:])\n",
    "#    next_adversary_location=(Game_Grid_Inv[y][choose[0]+1][1],Game_Grid_Inv[y][choose[0]+1][0])\n",
    "#    next_adversary_loc=int(Game_Grid[y][next_adversary_location[1],next_adversary_location[0]])\n",
    "#    obs_trans_matrix[current_adversary_loc-1,next_adversary_loc-1] += 1\n",
    "#    current_adversary_loc = next_adversary_loc\n",
    "#    current_adversary_location = next_adversary_location\n",
    "#    \n",
    "#for i in range(m):\n",
    "#    if np.sum(obs_trans_matrix[i,:]) != 0:\n",
    "#        obs_trans_matrix[i, ] = obs_trans_matrix[i, ]/np.sum(obs_trans_matrix[i,:])\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "reward = {}\n",
    "importance=100000\n",
    "n_success = 0\n",
    "total_reward = 0\n",
    "total_regret = 0\n",
    "\n",
    "average_agent_heatmap = {}\n",
    "for j in range(0,maze_size):\n",
    "    for k in range(0,maze_size):\n",
    "        average_agent_heatmap[(j,k)] = 1\n",
    "\n",
    "for e in range(25):\n",
    "    agent_heatmap = {}\n",
    "    for j in range(0,maze_size):\n",
    "        for k in range(0,maze_size):\n",
    "            agent_heatmap[(j,k)] = 0\n",
    "    reward[e] = 0\n",
    "    done=False\n",
    "    end=False\n",
    "    captured=[False]*len(rewards)\n",
    "    current_agent_location=starting_agent_location\n",
    "    current_adversary_location = []  \n",
    "    for i in range(0,len(rewards)):\n",
    "        current_adversary_location.append((rewards[i][1]-1,rewards[i][0]-1))\n",
    "    current_time=0\n",
    "    while True:\n",
    "        #print(\"Current Time:\" + str(current_time))\n",
    "        ####################\n",
    "        ##Printing details##\n",
    "        ####################\n",
    "        current_adversary_loc = []\n",
    "        for z in range(0,len(rewards)):\n",
    "            current_adversary_loc.append(int(Game_Grid[z][current_adversary_location[z][1],current_adversary_location[z][0]]))\n",
    "        #print(\"Current agent location: \"+str(current_agent_location))\n",
    "        #print(\"Current adversary location: \"+str(current_adversary_location))\n",
    "        #print(\"Current adversary loc:\" + str(current_adversary_loc))\n",
    "        #print(\"Current time: \"+str(current_time))\n",
    "        \n",
    "        agent_heatmap[(current_agent_location[1],current_agent_location[0])] -= .2\n",
    "\n",
    "        for z in range(0,len(rewards)):\n",
    "            if current_agent_location==current_adversary_location[z]:\n",
    "                end = True\n",
    "        \n",
    "        if end==True:\n",
    "            break\n",
    "                \n",
    "        \n",
    "        if done==True:\n",
    "            average_agent_heatmap = Counter(average_agent_heatmap) + Counter(agent_heatmap)\n",
    "            n_success += 1\n",
    "            total_reward += reward[e]\n",
    "            total_regret += (475-reward[e])\n",
    "            #print(reward[e])\n",
    "        \n",
    "            break\n",
    "\n",
    "        ######################\n",
    "        ##Setting up rewards##\n",
    "        ######################\n",
    "        #print(\"captured:\"+str(captured))\n",
    "        if all(captured):\n",
    "            destination=dest\n",
    "        else:\n",
    "            #rew = random.choice((np.where(np.array((captured))==False)[0]))\n",
    "            #print(captured)\n",
    "            rew = (np.where(np.array((captured))==False)[0])[0]\n",
    "            destination = (rewards[rew][0],rewards[rew][1])\n",
    "        #print(\"destination:\"+ str(destination))\n",
    "        r={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            r[i] = 0\n",
    "        for z in range(0,len(rewards)):\n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                if d['reward'+str(z)]==True and not captured[z]:\n",
    "                    r[i]=maxReward\n",
    "\n",
    "        ########################\n",
    "        ##Setting up penalties##\n",
    "        ########################\n",
    "        p={}\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            p[i]={}\n",
    "            p[i][current_time]=0\n",
    "\n",
    "        for t in range(current_time, T):\n",
    "            for (i,d) in G.nodes(data=True):\n",
    "                p[i][t]=0\n",
    "        for t in range(current_time, T):\n",
    "            for z in range(0,len(rewards)):\n",
    "                original_probabilities = np.linalg.matrix_power(obs_trans_matrix, t+1-current_time)[current_adversary_loc[z]-1,] # probabilities using matrix power function \n",
    "                #for (i,d) in G.nodes(data=True):\n",
    "                #    if d['possible_adversary'+str(z)]==True:\n",
    "                #        p[i][t]=original_probabilities[int(Game_Grid[z][i[1]][i[0]])-1]\n",
    "                for i, s in enumerate(list(original_probabilities)):\n",
    "                    if s != 0:\n",
    "                        p[(Game_Grid_Inv[z][i+1][1],Game_Grid_Inv[z][i+1][0])][t] = original_probabilities[i]\n",
    "                        \n",
    "        \n",
    "\n",
    "        ####################\n",
    "        ##Setting up model##\n",
    "        ####################          \n",
    "        model=Model(\"model_time\"+str(current_time))\n",
    "        model.setParam('OutputFlag', 0) \n",
    "        x={}\n",
    "        y={}\n",
    "        for i in G.nodes():\n",
    "            y[i]={}\n",
    "            y[i][current_time]=model.addVar(vtype=GRB.BINARY, name=\"y\"+str(i)+\",\"+str(t))\n",
    "            for t in range(current_time, T):\n",
    "                y[i][t+1]=model.addVar(vtype=GRB.BINARY,obj=t-r[i]+importance*p[i][t], name=\"y\"+str(i)+\",\"+str(t))\n",
    "        for (i,j) in G.edges():\n",
    "            x[i,j]={}\n",
    "            x[j,i]={}\n",
    "            for t in range(current_time, T):\n",
    "                x[i,j][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(i)+\",\"+str(j)+\",\"+str(t))\n",
    "                x[j,i][t]=model.addVar(vtype=GRB.BINARY, name=\"x\"+str(j)+\",\"+str(i)+\",\"+str(t))\n",
    "\n",
    "\n",
    "        ################################\n",
    "        ## Setting up the constraints ##\n",
    "        model.addConstr(y[current_agent_location[0], current_agent_location[1]][current_time]==1) \n",
    "        ################################\n",
    "\n",
    "        # agent only be at one node at a time\n",
    "        for t in range(current_time, T):\n",
    "            #print(\"time:\"+str(t))\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for i in G.nodes()) <= 1)\n",
    "\n",
    "        # agent can only be in each node once when planning\n",
    "        for i in G.nodes():\n",
    "            model.addConstr(quicksum(y[i[0],i[1]][t] for t in range(current_time,T)) <= 1)\n",
    "\n",
    "        # The agent can and will only exit from the node it is currently in\n",
    "        for i in G.nodes():\n",
    "            for t in range(current_time+1, T):\n",
    "                model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==y[i][t])\n",
    "\n",
    "        # When an agent enters a node it must exit it as well\n",
    "        for i in G.nodes():\n",
    "            if i!=destination and i!=current_agent_location:\n",
    "                for t in range(current_time+1, T):\n",
    "                    model.addConstr(quicksum(x[j,i][t-1] for j in G[i])==quicksum(x[i,j][t] for j in G[i]))#, name=str(i))\n",
    "\n",
    "        # agent has to move every step\n",
    "        model.addConstr(quicksum(x[current_agent_location, j][current_time] for j in G[current_agent_location])==1)\n",
    "\n",
    "        #have to reach destination(might not be needed)\n",
    "        expr=LinExpr()\n",
    "        for j in G[destination]:\n",
    "            for t in range(current_time, T):\n",
    "                expr+=x[j, destination][t]\n",
    "        model.addConstr(expr==1)\n",
    "\n",
    "        #model.write(\"myModel\"+str(current_time)+\".lp\")\n",
    "\n",
    "        model.optimize()\n",
    "        next_location=-1\n",
    "        for t in range(current_time, T):\n",
    "            for (i,j) in G.edges():\n",
    "                if round(x[i,j][t].X)==1:\n",
    "                    #print(t)\n",
    "                    #print(i,j)\n",
    "                    if t==current_time:\n",
    "                        next_location=j\n",
    "                if round(x[j,i][t].X)==1:\n",
    "                    #print(t)\n",
    "                    #print(j,i)\n",
    "                    if t==current_time:\n",
    "                        next_location=i\n",
    "        for i in G.nodes():\n",
    "            if y[i][current_time].X==1:\n",
    "                #print(i)\n",
    "                continue\n",
    "\n",
    "        current_agent_location=next_location \n",
    "\n",
    "        #print(\"Next agent location: \" + str(next_location))\n",
    "        for (i,d) in G.nodes(data=True):\n",
    "            for z in range(0,len(rewards)):\n",
    "                if d['reward'+str(z)]==True:\n",
    "                    if str(i)==str(next_location):\n",
    "                        captured[z]=True\n",
    "                        #print(\"Just captured reward \" + str(z))\n",
    "\n",
    "\n",
    "\n",
    "        current_time+=1\n",
    "\n",
    "        ### THIS IS THE PART THAT NEEDS UPDATING ########\n",
    "        next_adversary_location = []\n",
    "        for z in range(0,len(rewards)):\n",
    "            choose=choices(range(0,8), Trans_Matrix[current_adversary_loc[z]-1,:])\n",
    "            #print(choose)\n",
    "            next_adversary_location.append((Game_Grid_Inv[z][choose[0]+1][1],Game_Grid_Inv[z][choose[0]+1][0]))\n",
    "        ########################################\n",
    "\n",
    "        for z in range(0,len(rewards)):\n",
    "            current_adversary_location[z]=next_adversary_location[z]\n",
    "            #print(\"Next adversary location: \" + str(next_adversary_location))\n",
    "\n",
    "        \n",
    "        if current_agent_location in current_adversary_location:\n",
    "            reward[e] -= 1000\n",
    "        elif current_agent_location in rewards:\n",
    "            if agent_heatmap[(current_agent_location[1],current_agent_location[0])] == 0:\n",
    "                reward[e] += 200\n",
    "            else:\n",
    "                reward[e] -= 1\n",
    "        elif current_agent_location == destination:\n",
    "            reward[e] += 100\n",
    "        else:\n",
    "            reward[e] -= 1\n",
    "            \n",
    "        \n",
    "\n",
    "        if current_agent_location==dest:\n",
    "            #if captured == True:\n",
    "                #print(\"Won Game\")\n",
    "                #zprint(reward)\n",
    "            done=True\n",
    "\n",
    "            \n",
    "print(\"time: \",time.time()-start_time)            \n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = maze_size\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.full(\n",
    "                        shape=(maze_size,maze_size),\n",
    "                        fill_value=1,\n",
    "                        dtype=np.int)\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)\n",
    "print(\"number of successes\",n_success)\n",
    "print(\"average reward:\",total_reward/n_success)\n",
    "print(\"average regret:\",475-(total_reward/n_success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEwklEQVR4nO3dMU6UWxyH4fPd2GliQzINhR0LmLsmV0DrDljBXQG7cAO4ADoLGhNKrL/bUFjIIIbzDe/keRIqzf+MwTdq83NZ13UAb98/x/4AwJ8RK0SIFSLEChFihQixQsS7l/zks7Oz9dOnT5M+yhg/f/4c79+/d/9I97d4w/3Dvn//Pu7v75ff/uC6rn/8td/v15m+fv3q/hHvb/GG+4c9Nvbb/vw1GCLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCHiRVOkwBjL8vul0Onvrs/8l4/LsnweY3weY4zdbre/vr6e9mEeHh7Ghw8f3D/S/S3eOIX7t7e30+5fXl6OdV3tBrt//DdO4f4YY+rXajcY2sQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsEGE32P1N3ziF+3aD19PYlC3f3+KNU7g/7AYDh4gVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIcJu8Cvfn7kpe35+Pu7u7qbdH2OMi4uL/PegfP/y8nLc3NzYDa5vyl5dXU3frD2F70H5/mNjdoOhTKwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKES+K9du3b2NZlmlfs+9v4akZydf42u/3U++vz2xIV8z+PXq0X9dz36BfR74/fvy4//Lly7QPM3vE2oD18d84haH13W437f6rjXyPyQPTs0es6wPQs+9v8cYpDK3PZOQbToBYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0S8qVhn7+JC2ZvaDbbre9z7W7xhN/iwzG7wKWzWlu9v8Ybd4MPsBsMJECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChN1g9zd9w27wYXaDH7l//DfsBh9mNxhOgFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIuwGu7/pGz9+/Bh3d3fT7p+fn0+/bzfYbvDR72/xxtXV1fRd39n3Z7IbDCdArBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRL9oN3u12++vr62kfpr67W7+/xRvuH/Zqu8GPm6bT1Hd36/e3eMP9w+wGwwkQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsULEu+d+wq+7wWOMh2VZbid+nrMxxr37R7u/xRvuH3bx1A88O/K9pWVZbtZ1/df949zf4g33//6+vwZDhFgh4q3F+p/7R72/xRvu/+X9N/VvVuBpb+1PVuAJYoUIsUKEWCFCrBDxPz4OZ+iDnq4GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.grid('on')\n",
    "n = maze_size\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, n, 1))\n",
    "ax.set_yticks(np.arange(0.5, n, 1))\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "canvas = maze = np.full(\n",
    "                        shape=(maze_size,maze_size),\n",
    "                        fill_value=1,\n",
    "                        dtype=np.int)\n",
    "for cell in average_agent_heatmap:\n",
    "    canvas[cell] = average_agent_heatmap[cell]\n",
    "img1 = plt.imshow(canvas, interpolation='none', cmap='gray', vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
